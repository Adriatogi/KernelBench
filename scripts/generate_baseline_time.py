import torch
import numpy as np
from src.eval import (
    load_original_model_and_inputs,
    time_execution_with_cuda_event,
    get_timing_stats,
    set_seed,
    fetch_ref_arch_from_problem_id,
)
from src.dataset import construct_problem_dataset_from_problem_dir
import os
import logging
import json
from torch.profiler import profile, record_function, ProfilerActivity

device = torch.device("cuda:0")

REPO_TOP_PATH = os.path.abspath(
    os.path.join(
        os.path.dirname(__file__),
        "..",
    )
)
KERNEL_BENCH_PATH = os.path.join(REPO_TOP_PATH, "KernelBench")


def fetch_ref_arch_from_level_problem_id(level_num, problem_id, with_name=False):
    PROBLEM_DIR = os.path.join(KERNEL_BENCH_PATH, "level" + str(level_num))
    dataset = construct_problem_dataset_from_problem_dir(PROBLEM_DIR)
    return fetch_ref_arch_from_problem_id(problem_id, dataset, with_name)

def run_profile(level_num, problem_id, num_trials=10):
    ref_arch_name, ref_arch_src = fetch_ref_arch_from_level_problem_id(
        level_num, problem_id, with_name=True
    )
    ref_arch_name = ref_arch_name.split("/")[-1]
    context = {}
    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(
        ref_arch_src, context
    )
    # try:
    with torch.no_grad():
        profiling_scheduler = torch.profiler.schedule(
            wait=1,
            warmup=2,
            active=7,
        )
        torch.cuda.synchronize(device=device)
        set_seed(42)
        inputs = get_inputs()
        set_seed(42)
        init_inputs = get_init_inputs()
        inputs = [
            x.cuda(device=device) if isinstance(x, torch.Tensor) else x
            for x in inputs
        ]
        init_inputs = [
            x.cuda(device=device) if isinstance(x, torch.Tensor) else x
            for x in init_inputs
        ]
        
        # Create base model
        model = Model(*init_inputs)
        model = model.cuda(device=device)
        
        # Profile non-compiled model
        with profile(
            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
            schedule=profiling_scheduler
        ) as prof:
            with record_function("non_compiled_forward"):
                for _ in range(num_trials):
                    model(*inputs)
                    prof.step()
        print(f"\nProfiling results for non-compiled model:")
        print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
        
        # Profile compiled model
        model_compiled = torch.compile(model)
        with profile(
            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]
        ) as prof_compiled:
            with record_function("compiled_forward"):
                for _ in range(num_trials):
                    model_compiled(*inputs)
                    prof_compiled.step()
        print(f"\nProfiling results for compiled model:")
        print(prof_compiled.key_averages().table(sort_by="cuda_time_total", row_limit=10))


        prof.export_chrome_trace("trace_non_compiled.json")
        prof_compiled.export_chrome_trace("trace_compiled.json")

    # except Exception as e:
        # print(f"[Eval] Error in Measuring Performance: {e}")


def get_time(level_num, problem_id, num_trials=100, torch_compile=False):
    ref_arch_name, ref_arch_src = fetch_ref_arch_from_level_problem_id(
        level_num, problem_id, with_name=True
    )
    ref_arch_name = ref_arch_name.split("/")[-1]
    context = {}
    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(
        ref_arch_src, context
    )
    try:
        with torch.no_grad():
            torch.cuda.synchronize(device=device)
            set_seed(42)
            inputs = get_inputs()
            set_seed(42)
            init_inputs = get_init_inputs()
            inputs = [
                x.cuda(device=device) if isinstance(x, torch.Tensor) else x
                for x in inputs
            ]
            init_inputs = [
                x.cuda(device=device) if isinstance(x, torch.Tensor) else x
                for x in init_inputs
            ]
            model = Model(*init_inputs)
            if torch_compile:
                model = torch.compile(model)
            model = model.cuda(device=device)
            torch.cuda.synchronize(device=device)
            elapsed_times = time_execution_with_cuda_event(
                model, *inputs, num_trials=num_trials, verbose=False, device=device
            )
            runtime_stats = get_timing_stats(elapsed_times, device=device)
            # json_results[f"level{level_num}"][ref_arch_name] = runtime_stats
            print(f"{ref_arch_name} {runtime_stats}")
            return (ref_arch_name, runtime_stats)
    except Exception as e:
        print(f"[Eval] Error in Measuring Performance: {e}")


def get_torch_compile_triton(level_num, problem_id):
    """
    Get the triton code generated by torch compile for a particular problem
    """
    ref_arch_name, ref_arch_src = fetch_ref_arch_from_level_problem_id(
        level_num, problem_id, with_name=True
    )
    ref_arch_name = ref_arch_name.split("/")[-1]
    context = {}
    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(
        ref_arch_src, context
    )
    try:
        with torch.no_grad():
            torch.cuda.synchronize(device=device)
            set_seed(42)
            inputs = get_inputs()
            set_seed(42)
            init_inputs = get_init_inputs()
            inputs = [
                x.cuda(device=device) if isinstance(x, torch.Tensor) else x
                for x in inputs
            ]
            init_inputs = [
                x.cuda(device=device) if isinstance(x, torch.Tensor) else x
                for x in init_inputs
            ]
            model = Model(*init_inputs)

            # output triton code
            log_file = f"results/triton_code/level{level_num}_problem{problem_id}_triton.log"
            # os.makedirs(os.path.dirname(log_file), exist_ok=True)
            # logging.basicConfig(filename=log_file, level=logging.INFO)
            # TODO: Figure out a way to save to a file 

            torch._logging.set_logs(output_code=True)
            # Call torch compile
            model = torch.compile(model)
            model = model.cuda(device=device)
            torch.cuda.synchronize(device=device)
            elapsed_times = time_execution_with_cuda_event(
                model, *inputs, num_trials=1, verbose=False, device=device
            )
            # runtime_stats = get_timing_stats(elapsed_times, device=device)
            # json_results[f"level{level_num}"][ref_arch_name] = runtime_stats
            # print(f"{ref_arch_name} {runtime_stats}")
            return (ref_arch_name)
    except Exception as e:
        print(f"[Eval] Error in Measuring Performance: {e}")


def record_baseline_times():
    for torch_compile in [True, False]:

        json_results = {}
        PROBLEM_DIR_LEVEL1 = "KernelBench/level1"
        dataset_level1 = construct_problem_dataset_from_problem_dir(PROBLEM_DIR_LEVEL1)
        json_results["level1"] = {}
        for problem_id in range(len(dataset_level1)):
            ref_arch_name, runtime_stats = get_time(
                1, problem_id, torch_compile=torch_compile
            )
            json_results["level1"][ref_arch_name] = runtime_stats

        PROBLEM_DIR_LEVEL2 = "KernelBench/level2"
        dataset_level2 = construct_problem_dataset_from_problem_dir(PROBLEM_DIR_LEVEL2)
        json_results["level2"] = {}
        for problem_id in range(len(dataset_level2)):
            ref_arch_name, runtime_stats = get_time(
                2, problem_id, torch_compile=torch_compile
            )
            json_results["level2"][ref_arch_name] = runtime_stats

        PROBLEM_DIR_LEVEL3 = "KernelBench/level3"
        dataset_level3 = construct_problem_dataset_from_problem_dir(PROBLEM_DIR_LEVEL3)
        json_results["level3"] = {}
        for problem_id in range(len(dataset_level3)):
            ref_arch_name, runtime_stats = get_time(
                3, problem_id, torch_compile=torch_compile
            )
            json_results["level3"][ref_arch_name] = runtime_stats

        if torch_compile:
            save_path = f"results/timing/baseline_time_torch_compile.json"
        else:
            save_path = f"results/timing/baseline_time.json"
        with open(save_path, "w") as f:
            json.dump(json_results, f)

if __name__ == "__main__":
    # get_torch_compile_triton(1, 12)
    # record_baseline_times()

    run_profile(2, 43)
    # get_time(2, 43, torch_compile=False)
    # get_time(2, 43, torch_compile=True)

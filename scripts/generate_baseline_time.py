import torch
import numpy as np
from src.eval import (
    load_original_model_and_inputs,
    time_execution_with_cuda_event,
    get_timing_stats,
    set_seed,
    fetch_ref_arch_from_problem_id,
)
from src.dataset import construct_problem_dataset_from_problem_dir
import os
import logging
import json

device = torch.device("cuda:0")

REPO_TOP_PATH = os.path.abspath(
    os.path.join(
        os.path.dirname(__file__),
        "..",
    )
)
KERNEL_BENCH_PATH = os.path.join(REPO_TOP_PATH, "KernelBench")


def fetch_ref_arch_from_level_problem_id(level_num, problem_id, with_name=False):
    PROBLEM_DIR = os.path.join(KERNEL_BENCH_PATH, "level" + str(level_num))
    dataset = construct_problem_dataset_from_problem_dir(PROBLEM_DIR)
    return fetch_ref_arch_from_problem_id(problem_id, dataset, with_name)


def get_time(level_num, problem_id, num_trials=100, torch_compile=False):
    ref_arch_name, ref_arch_src = fetch_ref_arch_from_level_problem_id(
        level_num, problem_id, with_name=True
    )
    ref_arch_name = ref_arch_name.split("/")[-1]
    context = {}
    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(
        ref_arch_src, context
    )
    try:
        with torch.no_grad():
            torch.cuda.synchronize(device=device)
            set_seed(42)
            inputs = get_inputs()
            set_seed(42)
            init_inputs = get_init_inputs()
            inputs = [
                x.cuda(device=device) if isinstance(x, torch.Tensor) else x
                for x in inputs
            ]
            init_inputs = [
                x.cuda(device=device) if isinstance(x, torch.Tensor) else x
                for x in init_inputs
            ]
            model = Model(*init_inputs)
            if torch_compile:
                model = torch.compile(model)
            model = model.cuda(device=device)
            torch.cuda.synchronize(device=device)
            elapsed_times = time_execution_with_cuda_event(
                model, *inputs, num_trials=num_trials, verbose=False, device=device
            )
            runtime_stats = get_timing_stats(elapsed_times, device=device)
            # json_results[f"level{level_num}"][ref_arch_name] = runtime_stats
            print(f"{ref_arch_name} {runtime_stats}")
            return (ref_arch_name, runtime_stats)
    except Exception as e:
        print(f"[Eval] Error in Measuring Performance: {e}")


def get_torch_compile_triton(level_num, problem_id):
    """
    Get the triton code generated by torch compile for a particular problem
    """
    ref_arch_name, ref_arch_src = fetch_ref_arch_from_level_problem_id(
        level_num, problem_id, with_name=True
    )
    ref_arch_name = ref_arch_name.split("/")[-1]
    context = {}
    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(
        ref_arch_src, context
    )
    try:
        with torch.no_grad():
            torch.cuda.synchronize(device=device)
            set_seed(42)
            inputs = get_inputs()
            set_seed(42)
            init_inputs = get_init_inputs()
            inputs = [
                x.cuda(device=device) if isinstance(x, torch.Tensor) else x
                for x in inputs
            ]
            init_inputs = [
                x.cuda(device=device) if isinstance(x, torch.Tensor) else x
                for x in init_inputs
            ]
            model = Model(*init_inputs)

            # output triton code
            log_file = f"results/triton_code/level{level_num}_problem{problem_id}_triton.log"
            # os.makedirs(os.path.dirname(log_file), exist_ok=True)
            # logging.basicConfig(filename=log_file, level=logging.INFO)
            # TODO: Figure out a way to save to a file 

            torch._logging.set_logs(output_code=True)
            # Call torch compile
            model = torch.compile(model)
            model = model.cuda(device=device)
            torch.cuda.synchronize(device=device)
            elapsed_times = time_execution_with_cuda_event(
                model, *inputs, num_trials=1, verbose=False, device=device
            )
            # runtime_stats = get_timing_stats(elapsed_times, device=device)
            # json_results[f"level{level_num}"][ref_arch_name] = runtime_stats
            # print(f"{ref_arch_name} {runtime_stats}")
            return (ref_arch_name)
    except Exception as e:
        print(f"[Eval] Error in Measuring Performance: {e}")


def record_baseline_times():
    for torch_compile in [True, False]:

        json_results = {}
        PROBLEM_DIR_LEVEL1 = "KernelBench/level1"
        dataset_level1 = construct_problem_dataset_from_problem_dir(PROBLEM_DIR_LEVEL1)
        json_results["level1"] = {}
        for problem_id in range(len(dataset_level1)):
            ref_arch_name, runtime_stats = get_time(
                1, problem_id, torch_compile=torch_compile
            )
            json_results["level1"][ref_arch_name] = runtime_stats

        PROBLEM_DIR_LEVEL2 = "KernelBench/level2"
        dataset_level2 = construct_problem_dataset_from_problem_dir(PROBLEM_DIR_LEVEL2)
        json_results["level2"] = {}
        for problem_id in range(len(dataset_level2)):
            ref_arch_name, runtime_stats = get_time(
                2, problem_id, torch_compile=torch_compile
            )
            json_results["level2"][ref_arch_name] = runtime_stats

        PROBLEM_DIR_LEVEL3 = "KernelBench/level3"
        dataset_level3 = construct_problem_dataset_from_problem_dir(PROBLEM_DIR_LEVEL3)
        json_results["level3"] = {}
        for problem_id in range(len(dataset_level3)):
            ref_arch_name, runtime_stats = get_time(
                3, problem_id, torch_compile=torch_compile
            )
            json_results["level3"][ref_arch_name] = runtime_stats

        if torch_compile:
            save_path = f"results/timing/baseline_time_torch_compile.json"
        else:
            save_path = f"results/timing/baseline_time.json"
        with open(save_path, "w") as f:
            json.dump(json_results, f)

if __name__ == "__main__":
    # get_torch_compile_triton(1, 12)
    record_baseline_times()
